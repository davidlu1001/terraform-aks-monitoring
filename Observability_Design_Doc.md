# AKS Observability Design for Immuta Services

## Document Version History

| Version | Date               | Author           | Key Changes                                                                                                                                                  |
| :------ | :----------------- | :--------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 4.1     | August 2025        | Immuta Admin     | Initial comprehensive design with 55 alerts.                                                                                                                 |
| **5.0** | **September 2025** | **Immuta Admin** | **Reviewed implementation feasibility, identified application monitoring gap, refined alert definitions, and added recommendations for Terraform metadata.** |

## Summary

This document outlines a comprehensive observability strategy for Immuta services running on Azure Kubernetes Service (AKS), leveraging Azure Monitor and Container Insights. The design incorporates Azure's recommended metric alerts, follows industry best practices, and is optimized for New Zealand (Auckland) operations across QA and Production environments with clear alert taxonomy and operational procedures. This version reflects an analysis of the current Terraform implementation and provides recommendations to achieve full design coverage.

## Design Objectives

- **Proactive Detection**: Identify issues before they impact users through comprehensive monitoring.
- **Cost Optimization**: Efficient use of Azure Monitor capabilities with intelligent data collection.
- **Scalability**: Support growing Immuta workloads with minimal operational overhead.
- **Operational Clarity**: Clear alert classification and actionable metadata for efficient incident response.
- **Regional Optimization**: Auckland timezone considerations for maintenance and alerting.
- **Full Implementation**: Ensure the designed monitoring strategy is fully reflected in the infrastructure-as-code deployment.

## Architecture Overview

### Approved Monitoring Stack
- **Azure Monitor**: Core telemetry platform and alerting engine.
- **Container Insights**: AKS-specific monitoring with enhanced container visibility.
- **Log Analytics Workspace**: Centralized log storage and analysis.
- **Action Groups**: Role-based notification system (Email, Teams).
- **Alert Processing Rules**: Intelligent suppression and grouping.

### Observability Data Flow
```

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     AKS Cluster Sources                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Immuta Pods   ‚îÇ ‚Ä¢ Node Metrics    ‚îÇ ‚Ä¢ Control Plane     ‚îÇ
‚îÇ ‚Ä¢ App Logs      ‚îÇ ‚Ä¢ System Events   ‚îÇ ‚Ä¢ API Server        ‚îÇ
‚îÇ ‚Ä¢ Health Checks ‚îÇ ‚Ä¢ Resource Usage  ‚îÇ ‚Ä¢ Etcd/Scheduler    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ
‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Container Insights Collection                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Performance   ‚îÇ ‚Ä¢ Health Data     ‚îÇ ‚Ä¢ Log Aggregation   ‚îÇ
‚îÇ ‚Ä¢ Metrics Store ‚îÇ ‚Ä¢ Event Capture   ‚îÇ ‚Ä¢ Alert Evaluation  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ
‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Log Analytics Workspace                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ ContainerLogv2‚îÇ ‚Ä¢ Perf Tables     ‚îÇ ‚Ä¢ KubeEvents        ‚îÇ
‚îÇ ‚Ä¢ InsightsMetrics‚îÇ ‚Ä¢ Alert Queries  ‚îÇ ‚Ä¢ Custom Analytics  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ
‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Alert Classification                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Symptom Alerts‚îÇ ‚Ä¢ Cause Alerts    ‚îÇ ‚Ä¢ Severity Levels   ‚îÇ
‚îÇ ‚Ä¢ Customer Impact‚îÇ ‚Ä¢ Root Cause     ‚îÇ ‚Ä¢ Escalation Paths  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ
‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Notification & Response                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Email Groups  ‚îÇ ‚Ä¢ Time-aware      ‚îÇ ‚Ä¢ Runbook Links     ‚îÇ
‚îÇ ‚Ä¢ Role-based    ‚îÇ ‚Ä¢ Suppression     ‚îÇ ‚Ä¢ Escalation Matrix ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

```

## üìä Alert Taxonomy & Classification

### Alert Structure Overview
Based on SRE best practices, alerts are organized into **5 primary categories** with **56 total alerts**:

```

Total Alerts (56 alerts)
‚îú‚îÄ‚îÄ Azure Resource Level (4 alerts) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 7%
‚îú‚îÄ‚îÄ Kubernetes Infrastructure (30 alerts) ‚îÄ‚îÄ‚îÄ 54%
‚îú‚îÄ‚îÄ Control Plane Health (7 alerts) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 12%
‚îú‚îÄ‚îÄ Platform Workload Health (15 alerts) ‚îÄ‚îÄ‚îÄ‚îÄ 27%
‚îî‚îÄ‚îÄ Immuta Application Health (To Be Implemented)

````

### Alert Type Classification
- **Symptom Alerts** (29 alerts - 52%): Direct customer/service impact.
- **Cause Alerts** (27 alerts - 48%): Root cause identification.

### Severity Distribution
- **Critical (P0)**: 2 alerts (4%) - Service down scenarios.
- **High (P1)**: 8 alerts (14%) - Significant impact.
- **Warning (P2)**: 46 alerts (82%) - Degradation/prevention.

## üîç Detailed Alert Categories

### 1. Azure Resource Level Alerts (4 alerts)
**Purpose**: Monitor Azure subscription and regional service health impacting AKS clusters.

| Alert Name                         | Description                       | Threshold          | Window     | Severity | Type    |
| ---------------------------------- | --------------------------------- | ------------------ | ---------- | -------- | ------- |
| **Azure Subscription Quota Usage** | Resource quota approaching limits | >80% quota usage   | 1 hour     | Warning  | Cause   |
| **Azure Service Health Incidents** | Regional service incidents        | Any incident       | 5 minutes  | High     | Symptom |
| **Cost Anomaly Detection**         | Unusual spending patterns         | >50% cost increase | 6 hours    | Warning  | Cause   |
| **Network Security Group Blocks**  | High connection blocks by NSGs    | >100 blocks/30min  | 30 minutes | Warning  | Cause   |

### 2. Kubernetes Infrastructure Alerts (30 alerts)
**Purpose**: Monitor underlying AKS cluster infrastructure including nodes and pods.

#### 2.1 Node Infrastructure Health (12 alerts)
| Alert Name                     | Description                               | Threshold (QA/Prod)      | Window | Severity | Type    |
| ------------------------------ | ----------------------------------------- | ------------------------ | ------ | -------- | ------- |
| **KubeNodeUnreachable**        | Node marked unreachable                   | Any unreachable          | 15 min | Warning  | Symptom |
| **KubeNodeReadinessFlapping**  | **(New)** Node status changing frequently | >3 changes/15min         | 15 min | Warning  | Cause   |
| **KubeNodeDiskUsageHigh**      | Node disk usage high                      | >80% usage               | 15 min | Warning  | Cause   |
| **Node CPU Warning**           | Elevated CPU usage                        | 80%/75%                  | 15 min | Warning  | Cause   |
| **Node CPU Critical**          | Critical CPU usage                        | 90%/85%                  | 10 min | High     | Cause   |
| **Node Memory Warning**        | Elevated memory usage                     | 85%/80%                  | 15 min | Warning  | Cause   |
| **Node Memory Critical**       | Critical memory usage                     | 95%/90%                  | 15 min | High     | Cause   |
| **Node Network I/O High**      | High network traffic                      | >100MB/s sustained       | 10 min | Warning  | Cause   |
| **Node File Descriptor Usage** | FD exhaustion risk                        | >80% fd usage            | 10 min | Warning  | Cause   |
| **Node Pressure Events**       | Resource pressure                         | Memory/Disk/PID pressure | 5 min  | High     | Symptom |
| **Kubelet Health Issues**      | Kubelet service problems                  | Kubelet not ready        | 10 min | High     | Symptom |

#### 2.2 Pod Infrastructure Health (5 alerts)
| Alert Name                           | Description                           | Threshold (QA/Prod) | Window | Severity | Type    |
| ------------------------------------ | ------------------------------------- | ------------------- | ------ | -------- | ------- |
| **Pod CPU Usage High vs Request**    | CPU usage high relative to request    | >90%/85% requests   | 10 min | Warning  | Cause   |
| **Pod Memory Usage High vs Request** | Memory usage high relative to request | >85%/80% requests   | 10 min | Warning  | Cause   |
| **Pod Restart Rate High**            | High restart frequency                | >5/3 per hour       | 30 min | Warning  | Symptom |
| **Pod Network Errors**               | Network connectivity issues           | >50 errors/10min    | 10 min | Warning  | Cause   |
| **Pod Storage Latency High**         | Storage I/O performance               | >500ms avg latency  | 15 min | Warning  | Cause   |

*Note: Pod resource alerts should primarily be measured against requests to signal potential under-provisioning. Alerts against limits are covered by OOMKilled events.*

#### 2.3 Cluster Resource Management (4 alerts)
| Alert Name                    | Description                     | Threshold        | Window | Severity | Type    |
| ----------------------------- | ------------------------------- | ---------------- | ------ | -------- | ------- |
| **KubeCPUQuotaOvercommit**    | CPU requests exceed capacity    | >150% overcommit | 5 min  | Warning  | Cause   |
| **KubeMemoryQuotaOvercommit** | Memory requests exceed capacity | >150% overcommit | 5 min  | Warning  | Cause   |
| **KubeQuotaAlmostFull**       | Quota approaching limits        | >90% usage       | 15 min | Warning  | Cause   |
| **KubeClientErrors**          | API request error rate high     | >1% error rate   | 15 min | Warning  | Symptom |

#### 2.4 Storage Infrastructure (4 alerts)
| Alert Name                              | Description               | Threshold              | Window | Severity | Type    |
| --------------------------------------- | ------------------------- | ---------------------- | ------ | -------- | ------- |
| **KubePersistentVolumeFillingUp**       | Volume filling prediction | <15% available + trend | 60 min | Warning  | Cause   |
| **KubePersistentVolumeInodesFillingUp** | Low inodes remaining      | <3% free inodes        | 15 min | Warning  | Cause   |
| **KubePersistentVolumeErrors**          | Volume in failed state    | Any failed/pending     | 5 min  | Warning  | Symptom |
| **Persistent Volume Failures**          | Mount/access failures     | Any mount failure      | 15 min | High     | Symptom |

#### 2.5 Pod Lifecycle Management (5 alerts)
| Alert Name                      | Description                | Threshold                   | Window | Severity | Type    |
| ------------------------------- | -------------------------- | --------------------------- | ------ | -------- | ------- |
| **KubePodContainerRestart**     | High restart rate          | >0 restarts/hour            | 15 min | Warning  | Symptom |
| **KubePodFailedState**          | Pods in failed state       | Any failed pods             | 5 min  | Warning  | Symptom |
| **KubePodNotReadyByController** | Pods not ready extended    | Pending/Unknown >15min      | 15 min | Warning  | Symptom |
| **KubeContainerWaiting**        | Container waiting extended | Waiting >1 hour             | 60 min | Warning  | Symptom |
| **Pod Unavailable Critical**    | Critical pods unavailable  | Any critical pod down >5min | 5 min  | Critical | Symptom |


### 3. Control Plane Health Alerts (7 alerts)
**Purpose**: Monitor Kubernetes master components managing the cluster.

| Alert Name                    | Description                  | Threshold (QA/Prod)  | Window | Severity | Type    |
| ----------------------------- | ---------------------------- | -------------------- | ------ | -------- | ------- |
| **API Server Latency High**   | API response latency         | >200ms/100ms P95     | 10 min | Warning  | Cause   |
| **API Server Error Rate**     | High API error rate          | >5% error rate       | 10 min | High     | Symptom |
| **API Server Request Rate**   | Unusual request volume       | >200% baseline       | 5 min  | Warning  | Cause   |
| **Etcd Health Issues**        | Etcd cluster problems        | Any etcd errors      | 5 min  | Critical | Symptom |
| **Etcd Latency High**         | Transaction latency elevated | >100ms/50ms P95      | 10 min | Warning  | Cause   |
| **Scheduler Queue Depth**     | Scheduler queue backup       | >50 pending pods     | 15 min | Warning  | Cause   |
| **Controller Manager Health** | Controller health issues     | Controller errors >2 | 10 min | High     | Symptom |

### 4. Platform Workload Health Alerts (15 alerts)
**Purpose**: Monitor Kubernetes workload patterns, resource management, and application lifecycle.

#### 4.1 Container Issues (3 alerts)
| Alert Name                      | Description            | Threshold               | Window | Severity | Type    |
| ------------------------------- | ---------------------- | ----------------------- | ------ | -------- | ------- |
| **KubePodCrashLooping**         | CrashLoopBackOff state | CrashLoopBackOff status | 15 min | Warning  | Symptom |
| **KubeContainerOOMKilledCount** | OOM killed containers  | Any OOM killed          | 5 min  | Warning  | Symptom |
| **ImagePullBackOff Detection**  | Image pull failures    | ImagePullBackOff status | 10 min | Warning  | Cause   |

#### 4.2 Workload Management (7 alerts)
| Alert Name                            | Description                     | Threshold           | Window  | Severity | Type    |
| ------------------------------------- | ------------------------------- | ------------------- | ------- | -------- | ------- |
| **KubeDeploymentReplicasMismatch**    | Deployment replica mismatch     | Spec ‚â† Available    | 15 min  | Warning  | Symptom |
| **KubeStatefulSetReplicasMismatch**   | StatefulSet replica mismatch    | Desired ‚â† Ready     | 15 min  | Warning  | Symptom |
| **KubeStatefulSetGenerationMismatch** | StatefulSet generation mismatch | Observed ‚â† Metadata | 15 min  | Warning  | Symptom |
| **KubeDaemonSetNotScheduled**         | DaemonSet not scheduled         | Desired > Current   | 15 min  | Warning  | Symptom |
| **KubeDaemonSetMisScheduled**         | DaemonSet incorrectly scheduled | Misscheduled >0     | 15 min  | Warning  | Symptom |
| **KubeJobFailed**                     | Job failed to complete          | Any failed jobs     | 15 min  | Warning  | Symptom |
| **KubeJobStale**                      | Job running too long            | >6 hours incomplete | 360 min | Warning  | Symptom |

#### 4.3 Horizontal Pod Autoscaler (2 alerts)
| Alert Name                  | Description                       | Threshold                | Window | Severity | Type    |
| --------------------------- | --------------------------------- | ------------------------ | ------ | -------- | ------- |
| **KubeHpaReplicasMismatch** | HPA not matching desired replicas | Desired ‚â† Current >15min | 15 min | Warning  | Symptom |
| **KubeHpaMaxedOut**         | HPA running at maximum replicas   | At max replicas >15min   | 15 min | Warning  | Symptom |

#### 4.4 Resource Management (3 alerts)
| Alert Name                      | Description                  | Threshold            | Window | Severity | Type    |
| ------------------------------- | ---------------------------- | -------------------- | ------ | -------- | ------- |
| **Resource Quota Exceeded**     | Namespace quota exceeded     | Any quota exceeded   | 15 min | Warning  | Symptom |
| **Admission Controller Denial** | Controller blocking requests | >3 denials/10min     | 15 min | Warning  | Cause   |
| **Failed Pod Scheduling**       | Unschedulable pods           | Unschedulable >10min | 10 min | High     | Symptom |

### 5. Immuta Application-Specific Monitoring (**Gap in Implementation**)
**Purpose**: Monitor the health, performance, and security of the Immuta application itself. **These alerts are defined but not yet implemented in Terraform.**

#### Application Health Monitoring
| Metric Category           | Source             | Alert Condition          | Threshold (QA/Prod) | Severity |
| ------------------------- | ------------------ | ------------------------ | ------------------- | -------- |
| **Error Rate**            | ContainerLogv2     | High application errors  | >2%/>1% error rate  | High     |
| **Response Time**         | ContainerLogv2     | Slow response patterns   | >5s/>3s P95         | Warning  |
| **Health Check Failures** | ContainerLogv2     | Health endpoint failures | >2 consecutive      | High     |
| **Service Availability**  | Container Insights | Pod unavailability       | Any pod down >5min  | Critical |

#### Security Monitoring
| Metric Category             | Source         | Alert Condition            | Threshold         | Severity |
| --------------------------- | -------------- | -------------------------- | ----------------- | -------- |
| **Authentication Failures** | ContainerLogv2 | Auth failure patterns      | >15/>10 per 15min | High     |
| **Unauthorized Access**     | ContainerLogv2 | 401/403 error spikes       | >20 per 10min     | High     |
| **Suspicious IP Activity**  | ContainerLogv2 | Multiple IPs with failures | >5 unique IPs     | High     |

#### Database Connectivity
| Metric Category                | Source         | Alert Condition         | Threshold      | Severity |
| ------------------------------ | -------------- | ----------------------- | -------------- | -------- |
| **Connection Errors**          | ContainerLogv2 | DB connection failures  | >2 per 10min   | Critical |
| **Connection Pool Exhaustion** | ContainerLogv2 | Pool exhaustion events  | Any occurrence | High     |
| **Query Timeouts**             | ContainerLogv2 | Database query timeouts | >5 per 15min   | Warning  |

## üö® Alert Configuration Strategy

### Alert Metadata and Actionability

To enhance operational clarity, all Terraform alert resources should include a `custom_properties` block. This metadata directly links the alert to this design document and associated runbooks.

**Example Terraform Implementation**:
```terraform
resource "azurerm_monitor_scheduled_query_rules_alert_v2" "pod_storage_latency" {
  # ... other properties
  action {
    action_groups = [azurerm_monitor_action_group.immuta_standard.id]
    
    custom_properties = {
      alert_name      = "Pod Storage Latency High"
      alert_type      = "Cause"
      severity_level  = "P2"
      runbook_url     = "[https://your-wiki.com/runbooks/aks-pod-storage-latency](https://your-wiki.com/runbooks/aks-pod-storage-latency)"
      design_doc      = "AKS Observability Design"
    }
  }
}
````

### Severity-Based Response Framework

#### Critical Alerts (Severity 0) - 2 alerts

  - **Response Time**: 5 minutes
  - **Escalation**: Immediate to all stakeholders
  - **Business Hours**: 24/7 alerting
  - **Examples**:
      - Etcd Health Issues
      - Pod Unavailable Critical

#### High Alerts (Severity 1) - 8 alerts

  - **Response Time**: 15 minutes
  - **Escalation**: On-call team + manager notification
  - **Business Hours**: 24/7 for production, business hours for QA
  - **Examples**:
      - Node CPU/Memory Critical
      - API Server Error Rate
      - Controller Manager Health

#### Warning Alerts (Severity 2) - 46 alerts

  - **Response Time**: 1 hour
  - **Escalation**: SRE team notification
  - **Business Hours**: Business hours only for QA
  - **Examples**:
      - Resource usage warnings
      - Performance degradation
      - Workload mismatches

### Environment-Specific Thresholds

#### Production Environment (Stricter Thresholds)

```yaml
# Node Resource Thresholds
node_cpu_warning: 75%        # vs 80% in QA
node_cpu_critical: 85%       # vs 90% in QA
node_memory_warning: 80%     # vs 85% in QA
node_memory_critical: 90%    # vs 95% in QA

# Application Thresholds
error_rate_warning: 1%       # vs 2% in QA
response_time_warning: 3s    # vs 5s in QA
auth_failure_threshold: 10   # vs 15 in QA

# Pod Management
pod_restart_threshold: 3     # vs 5 in QA
```

#### QA Environment (Relaxed Thresholds)

```yaml
# Node Resource Thresholds (More tolerant for testing)
node_cpu_warning: 80%
node_cpu_critical: 90%
node_memory_warning: 85%
node_memory_critical: 95%

# Application Thresholds (Allow more errors during testing)
error_rate_warning: 2%
response_time_warning: 5s
auth_failure_threshold: 15

# Pod Management (More restarts expected)
pod_restart_threshold: 5
```

## üõ°Ô∏è Alert Processing & Suppression Rules

### Auckland Timezone Optimization

#### Maintenance Window Suppression

```yaml
Schedule: Sunday 2:00 AM - 4:00 AM NZST
Scope: All alerts except Critical (Severity 0)
Purpose: Planned maintenance activities
Duration: 2 hours weekly
```

#### Business Hours Management

```yaml
Business Hours: Monday-Friday 8:00 AM - 6:00 PM NZST
After Hours (QA): Suppress Warning and Info alerts
Weekend (QA): Suppress Warning and Info alerts
Production: 24/7 alerting for High and Critical
```

#### Intelligent Suppression

  - **Duplicate Alert Prevention**: Same alert within 10 minutes
  - **Burst Protection**: Maximum 5 alerts per resource per hour
  - **Alert Correlation**: Group related alerts to prevent spam

## üîç Advanced Monitoring Queries

### Infrastructure Monitoring

```kql
// Node resource pressure detection
KubeNodeInventory
| join kind=inner (
    Perf
    | where ObjectName == "K8SNode"
    | where CounterName in ("cpuUsageNanoCores", "memoryWorkingSetBytes")
) on Computer
| extend 
    ResourceType = iff(CounterName == "cpuUsageNanoCores", "CPU", "Memory"),
    UsagePercent = iff(CounterName == "cpuUsageNanoCores", 
        CounterValue / 1000000000 * 100, 
        CounterValue / (MemoryCapacityBytes / 1024 / 1024 / 1024) * 100)
| summarize 
    AvgUsage = avg(UsagePercent),
    P95Usage = percentile(UsagePercent, 95)
    by Computer, ResourceType, bin(TimeGenerated, 15m)
| where P95Usage > 80
```

### Application Error Detection

```kql
// Immuta application error analysis
ContainerLogv2
| where ContainerName has_any ("immuta")
| where LogEntry has_any ("ERROR", "FATAL", "Exception")
| where LogEntry !has_any ("health", "liveness", "readiness", "test")
| extend 
    Severity = case(
        LogEntry has_any ("FATAL", "OutOfMemoryError"), "Critical",
        LogEntry has_any ("ERROR", "Exception"), "High",
        "Warning"
    ),
    ErrorType = case(
        LogEntry has "SQLException", "Database",
        LogEntry has "AuthenticationException", "Authentication",
        LogEntry has "TimeoutException", "Timeout",
        "General"
    )
| summarize 
    ErrorCount = count(), 
    UniqueErrors = dcount(LogEntry),
    ErrorTypes = make_set(ErrorType)
    by bin(TimeGenerated, 5m), Severity, ContainerName
| where ErrorCount > 3
```

### Security Event Analysis

```kql
// Authentication failure monitoring
ContainerLogv2
| where ContainerName has_any ("immuta")
| where LogEntry has_any ("authentication", "authorization", "login")
| extend 
    IsFailure = LogEntry has_any ("failed", "denied", "unauthorized", "401", "403"),
    IPAddress = extract(@"(?:ip|client)[=:\s]+(\d+\.\d+\.\d+\.\d+)", 1, LogEntry),
    Username = extract(@"(?:user|username|email)[=:\s]+([^\s,]+)", 1, LogEntry)
| where isnotnull(IPAddress)
| summarize 
    TotalAttempts = count(),
    FailedAttempts = countif(IsFailure),
    FailureRate = (countif(IsFailure) * 100.0) / count()
    by bin(TimeGenerated, 15m), IPAddress
| where FailedAttempts > 10 or (FailureRate > 50 and TotalAttempts > 5)
```

## üìà Cost Optimization Strategy

### Data Collection Optimization

  - **Selective Collection**: Focus on Immuta-related logs and critical infrastructure
  - **Namespace Filtering**: Exclude system namespaces (kube-system, kube-public)
  - **Log Level Filtering**: Exclude debug/trace logs in production
  - **Retention Policies**:
      - Production: 90 days retention
      - QA: 30 days retention

### Cost Monitoring

```kql
// Daily ingestion cost analysis
Usage
| where TimeGenerated > ago(30d)
| where IsBillable == true
| summarize 
    IngestedGB = sum(Quantity) / 1024,
    EstimatedCostUSD = sum(Quantity) * 2.30 / 1024
    by DataType, bin(TimeGenerated, 1d)
| order by IngestedGB desc
```

## üîß Implementation Roadmap

### Phase 1: Foundation (Week 1-2) - ‚úÖ **Complete**

  - Deploy Azure recommended alerts (cluster, node, pod levels)
  - Configure action groups and email notifications
  - Set up Log Analytics workspace with retention policies
  - Implement core infrastructure monitoring

### Phase 2: Platform Health (Week 3-4) - ‚úÖ **Complete**

  - Deploy control plane health monitoring
  - Configure platform workload health alerts
  - Implement storage infrastructure monitoring
  - Set up alert processing rules for Auckland timezone

### Phase 3: Application Focus (Week 5-6) - ‚ö†Ô∏è **In Progress / Blocked**

  - **(New Priority)** Deploy Immuta-specific log query alerts (Error Rate, Response Time).
  - **(New Priority)** Implement security monitoring for authentication events.
  - **(New Priority)** Set up database connectivity monitoring.

### Phase 4: Optimization (Week 7-8) - üîÑ **In Progress**

  - Fine-tune alert thresholds based on real data to align with this document.
  - **(New)** Implement `KubeNodeReadinessFlapping` alert.
  - **(New)** Consolidate all Terraform alert definitions into a single, unified module.
  - **(New)** Systematically add `custom_properties` metadata to all alert rules.
  - Create and link comprehensive runbooks.
  - Conduct team training sessions.

## üéØ Success Metrics & KPIs

### Monitoring Effectiveness

  - **Mean Time to Detection (MTTD)**: Target \< 5 minutes for critical issues
  - **Mean Time to Resolution (MTTR)**: Target \< 30 minutes for high priority
  - **Alert Accuracy**: Target \> 95% true positive rate
  - **Coverage Completeness**: 100% of critical components monitored

### Operational Excellence

  - **Service Availability**: 99.9% uptime target
  - **Application Performance**: \<3s P95 response time for Production
  - **Error Rate**: \<0.1% for critical operations
  - **Security Incidents**: 0 undetected security events

### Cost Efficiency

  - **Cost per GB**: Monitor Log Analytics efficiency
  - **ROI**: Cost savings from prevented incidents
  - **Resource Optimization**: Reduce over-provisioned resources by 20%

## üìö Operational Procedures

### Daily Operations

  - **Morning Health Check**: Review overnight alerts and system status
  - **Alert Triage**: Prioritize and assign alerts based on severity
  - **Performance Review**: Analyze application performance trends
  - **Cost Monitoring**: Review daily ingestion and costs

### Weekly Operations

  - **Alert Effectiveness Review**: Analyze false positive rates and coverage
  - **Threshold Tuning**: Adjust alert thresholds based on trends
  - **Capacity Planning**: Review resource utilization trends
  - **Documentation Updates**: Update runbooks and procedures

### Monthly Operations

  - **Comprehensive Review**: Full monitoring strategy assessment
  - **Cost Optimization**: Review and optimize data collection
  - **Training Updates**: Update team training materials
  - **Disaster Recovery Testing**: Test monitoring during outages

-----

## Summary

This enhanced observability design provides:

### **Complete Alert Coverage**

  - **56 infrastructure and platform alerts** organized into 4 logical categories.
  - Comprehensive workload management including Deployments, StatefulSets, DaemonSets, Jobs, and HPA.
  - Symptom vs Cause classification for efficient incident response.
  - Environment-specific thresholds for QA and Production differentiation.

### **Operational Excellence**

  - Auckland timezone optimization for maintenance and business hours.
  - Intelligent alert suppression to prevent notification fatigue.
  - Clear escalation procedures and **actionable metadata** via `custom_properties`.
  - Comprehensive container performance monitoring.

### **Production Readiness & Implementation Status**

  - **Infrastructure/Platform**: \~95% implemented in Terraform across `aks_alerts.tf` and `aks_alerts_added.tf`.
  - **Application Monitoring**: **0% implemented. This is a critical gap that must be addressed in Phase 3.**
  - **Action Required**: Consolidate Terraform files, implement application-level alerts, and align all alert thresholds and metadata with this design document.